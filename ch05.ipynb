{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章: 係り受け解析\n",
    "    \n",
    "time required: \n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "夏目漱石の小説『吾輩は猫である』の文章（neko.txt）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 -1D 0/0 0.000000\n",
      "一\t名詞,数,*,*,*,*,一,イチ,イチ\n",
      "EOS\n",
      "EOS\n",
      "* 0 1D 0/1 0.000000\n",
      "吾輩\t名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "* 1 -1D 0/2 0.000000\n",
      "猫\t名詞,一般,*,*,*,*,猫,ネコ,ネコ\n",
      "で\t助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\n",
      "ある\t助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "EOS\n",
      "* 0 2D 0/1 -1.911675\n",
      "名前\t名詞,一般,*,*,*,*,名前,ナマエ,ナマエ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "* 1 2D 0/0 -1.911675\n",
      "まだ\t副詞,助詞類接続,*,*,*,*,まだ,マダ,マダ\n",
      "* 2 -1D 0/0 0.000000\n",
      "無い\t形容詞,自立,*,*,形容詞・アウオ段,基本形,無い,ナイ,ナイ\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# download\n",
    "if [ ! -f neko.txt ]; then\n",
    "    wget http://www.cl.ecei.tohoku.ac.jp/nlp100/data/neko.txt\n",
    "fi\n",
    "\n",
    "# call cabocha\n",
    "if [ ! -f neko.txt.cabocha ]; then\n",
    "    sed 's/^　//' neko.txt > neko_noindent.txt\n",
    "    cabocha -f 1 neko_noindent.txt > neko.txt.cabocha\n",
    "fi\n",
    "\n",
    "head -20 neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40. 係り受け解析結果の読み込み（形態素）\n",
    "\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Morph(object):\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '<Morpheme: %s (%s): %s %s>' % (self.surface, self.base, self.pos, self.pos1)\n",
    "    \n",
    "    #def __str__(self):\n",
    "    #    return self.surface\n",
    "    \n",
    "    @staticmethod\n",
    "    def construct(line):\n",
    "        surface = line.split('\\t', 1)\n",
    "        elem = surface[-1].split(',')\n",
    "        assert len(elem) > 5, elem\n",
    "        return Morph(surface[0], elem[6], elem[0], elem[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Morpheme: 名前 (名前): 名詞 一般>,\n",
       " <Morpheme: は (は): 助詞 係助詞>,\n",
       " <Morpheme: まだ (まだ): 副詞 助詞類接続>,\n",
       " <Morpheme: 無い (無い): 形容詞 自立>,\n",
       " <Morpheme: 。 (。): 記号 句点>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read mecab output file\n",
    "\n",
    "doc = []\n",
    "with open('neko.txt.cabocha', 'r', encoding='utf8') as f:\n",
    "    for s in f.read().split('EOS\\n'):\n",
    "        sent = []\n",
    "        for line in s.strip().split('\\n'):\n",
    "            if '\\t' in line:\n",
    "                sent.append(Morph.construct(line))\n",
    "        if sent:\n",
    "            doc.append(sent)\n",
    "\n",
    "doc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Chunk():\n",
    "    def __init__(self, morphs, dst, srcs):\n",
    "        self.morphs = morphs # list of Morphs\n",
    "        self.dst = dst         # int         (parent)\n",
    "        self.srcs = srcs       # list of int (children)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        surface = ' '.join([m.surface for m in self.morphs])\n",
    "        return '<Chunk: %s (%d)>' % (surface, self.dst)\n",
    "    \n",
    "    def surface(self, sep=' '):\n",
    "        return sep.join([m.surface for m in self.morphs if m.pos != u'記号'])\n",
    "    \n",
    "    @staticmethod\n",
    "    def construct(chunk, raw):\n",
    "        info = chunk.strip().split('\\n', 1)\n",
    "        idx = info[0].strip().split()\n",
    "        dst = int(idx[2][:-1])\n",
    "        m = re.findall(r'\\** \\d{1,3} '+idx[1]+'D', raw)\n",
    "        srcs = [int(i.split()[1]) for i in m]\n",
    "        morphs = []\n",
    "        for line in info[1].strip().split('\\n'):\n",
    "            morphs.append(Morph.construct(line))\n",
    "        return Chunk(morphs, dst, srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Chunk: 吾輩 は (5)>,\n",
       " <Chunk: ここ で (2)>,\n",
       " <Chunk: 始め て (3)>,\n",
       " <Chunk: 人間 という (4)>,\n",
       " <Chunk: もの を (5)>,\n",
       " <Chunk: 見 た 。 (-1)>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# read cabocha output file\n",
    "\n",
    "dependency = []\n",
    "with open('neko.txt.cabocha', 'r', encoding='utf8') as f:\n",
    "    for sent in f.read().split('EOS\\n'):\n",
    "        graph = []\n",
    "        for chunk in sent.strip().split('\\n*'):\n",
    "            if chunk:\n",
    "                graph.append(Chunk.construct('*'+chunk, sent))\n",
    "        dependency.append(graph)\n",
    "\n",
    "dependency[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 42. 係り元と係り先の文節の表示\n",
    "\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Chunk: 吾輩 は (5)>\n",
      "\thead: '見 た'\n",
      "\tdeps: \n",
      "\n",
      "<Chunk: ここ で (2)>\n",
      "\thead: '始め て'\n",
      "\tdeps: \n",
      "\n",
      "<Chunk: 始め て (3)>\n",
      "\thead: '人間 という'\n",
      "\tdeps: 'ここ で'\n",
      "\n",
      "<Chunk: 人間 という (4)>\n",
      "\thead: 'もの を'\n",
      "\tdeps: '始め て'\n",
      "\n",
      "<Chunk: もの を (5)>\n",
      "\thead: '見 た'\n",
      "\tdeps: '人間 という'\n",
      "\n",
      "<Chunk: 見 た 。 (-1)>\n",
      "\thead: 'ROOT'\n",
      "\tdeps: '吾輩 は'\t'もの を'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent = dependency[7]\n",
    "for dep in sent:\n",
    "    head = sent[dep.dst] if dep.dst > -1 else None\n",
    "    dependents = [sent[i] for i in dep.srcs]\n",
    "    print(dep)\n",
    "    print('\\thead: %r' % (head.surface() if head else 'ROOT'))\n",
    "    print('\\tdeps: %s\\n' % '\\t'.join([repr(d.surface()) for d in dependents]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "どこ で\t生れ た か\n",
      "見当 が\tつか ぬ\n",
      "所 で\t泣い て\n",
      "ニャーニャー\t泣い て\n",
      "いた事 だけ は\t記憶 し て いる\n",
      "吾輩 は\t見 た\n",
      "ここ で\t始め て\n",
      "もの を\t見 た\n",
      "あと で\t聞く と\n",
      "我々 を\t捕え て\n"
     ]
    }
   ],
   "source": [
    "for sent in dependency[:10]: # first 10 sentences only\n",
    "    for dep in sent:\n",
    "        if dep.dst > -1 and u'名詞' in [m.pos for m in dep.morphs]:\n",
    "                head = sent[dep.dst]\n",
    "                if u'動詞' in [m.pos for m in head.morphs]:\n",
    "                    print('%s\\t%s' % (dep.surface(), head.surface()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化\n",
    "\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "\n",
    "def build_graph(sent):\n",
    "    g = pydot.Dot(graph_type=\"digraph\")\n",
    "\n",
    "    # nodes\n",
    "    for i, chunk in enumerate(sent):\n",
    "        g.add_node(pydot.Node(str(i), label=chunk.surface()))\n",
    "\n",
    "    # edges\n",
    "    for i, chunk in enumerate(sent):\n",
    "        if chunk.dst > -1:\n",
    "            g.add_edge(pydot.Edge(str(i), str(chunk.dst)))\n",
    "    \n",
    "    return g.to_string()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display\n",
    "import subprocess\n",
    "\n",
    "# source: http://www.nltk.org/_modules/nltk/parse/dependencygraph.html\n",
    "\n",
    "def display_graph(dot_string):\n",
    "    process = subprocess.Popen(\n",
    "        ['dot', '-Tsvg'],\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        universal_newlines=True,\n",
    "    )\n",
    "    out, err = process.communicate(dot_string)\n",
    "    display(SVG(out))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"370pt\" viewBox=\"0.00 0.00 219.77 369.93\" width=\"220pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 365.931)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-365.931 215.773,-365.931 215.773,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 0 -->\n",
       "<g class=\"node\" id=\"node1\"><title>0</title>\n",
       "<ellipse cx=\"43.4871\" cy=\"-101.379\" fill=\"none\" rx=\"43.4741\" ry=\"21.5881\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"43.4871\" y=\"-101.589\">吾輩 は</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g class=\"node\" id=\"node6\"><title>5</title>\n",
       "<ellipse cx=\"95.4871\" cy=\"-21.7931\" fill=\"none\" rx=\"33.6754\" ry=\"21.5881\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"95.4871\" y=\"-22.0031\">見 た</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>0-&gt;5</title>\n",
       "<path d=\"M56.8784,-80.3988C62.9763,-71.3005 70.2951,-60.3805 76.9132,-50.506\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"79.8404,-52.4251 82.5005,-42.1696 74.0256,-48.5279 79.8404,-52.4251\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g class=\"node\" id=\"node2\"><title>1</title>\n",
       "<ellipse cx=\"148.487\" cy=\"-340.138\" fill=\"none\" rx=\"43.4741\" ry=\"21.5881\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148.487\" y=\"-340.348\">ここ で</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g class=\"node\" id=\"node3\"><title>2</title>\n",
       "<ellipse cx=\"148.487\" cy=\"-260.551\" fill=\"none\" rx=\"43.4741\" ry=\"21.5881\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148.487\" y=\"-260.761\">始め て</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>1-&gt;2</title>\n",
       "<path d=\"M148.487,-317.937C148.487,-310.09 148.487,-301.025 148.487,-292.489\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"151.987,-292.458 148.487,-282.458 144.987,-292.458 151.987,-292.458\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g class=\"node\" id=\"node4\"><title>3</title>\n",
       "<ellipse cx=\"148.487\" cy=\"-180.965\" fill=\"none\" rx=\"63.0728\" ry=\"21.5881\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148.487\" y=\"-181.175\">人間 という</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2-&gt;3</title>\n",
       "<path d=\"M148.487,-238.351C148.487,-230.504 148.487,-221.439 148.487,-212.903\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"151.987,-212.872 148.487,-202.872 144.987,-212.872 151.987,-212.872\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g class=\"node\" id=\"node5\"><title>4</title>\n",
       "<ellipse cx=\"148.487\" cy=\"-101.379\" fill=\"none\" rx=\"43.4741\" ry=\"21.5881\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"148.487\" y=\"-101.589\">もの を</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>3-&gt;4</title>\n",
       "<path d=\"M148.487,-158.765C148.487,-150.918 148.487,-141.853 148.487,-133.317\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"151.987,-133.286 148.487,-123.286 144.987,-133.286 151.987,-133.286\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>4-&gt;5</title>\n",
       "<path d=\"M134.838,-80.3988C128.576,-71.231 121.05,-60.2137 114.264,-50.28\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"117.072,-48.1863 108.541,-41.9032 111.292,-52.1348 117.072,-48.1863\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent = dependency[7]\n",
    "dependency_graph = build_graph(sent)\n",
    "display_graph(dependency_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "- 述語に係る助詞を格とする\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める    で\n",
    "見る    は を\n",
    "````\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "- 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "freq = defaultdict(list)\n",
    "with open('case_patterns.txt', 'w', encoding='utf8') as f:\n",
    "    for sent in dependency:\n",
    "        for dep in sent:\n",
    "            if u'動詞' in [m.pos for m in dep.morphs]:\n",
    "                predicate = [m for m in dep.morphs if m.pos==u'動詞'][0].base\n",
    "                cases = []\n",
    "                for dependent in [sent[i] for i in dep.srcs]:\n",
    "                    case = [m for m in dependent.morphs if m.pos==u'助詞']\n",
    "                    if case:\n",
    "                        cases.append(case[0].base)\n",
    "                f.write('%s\\t%s\\n' % (predicate, ' '.join(sorted(cases))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 584 云う\tと\n",
      " 443 する\tを\n",
      " 435 する\t\n",
      " 256 思う\tと\n",
      " 207 なる\tに\n",
      " 199 ある\tが\n",
      " 188 する\tに\n",
      " 179 見る\tて\n",
      " 134 する\tと\n",
      " 122 云う\t\n",
      " 117 する\tが\n",
      " 110 する\tに を\n",
      " 108 なる\t\n",
      " 102 する\tて を\n",
      "  94 見る\tを\n",
      "  94 ある\t\n",
      "  93 見える\tと\n",
      "  89 いる\t\n",
      "  88 する\tて\n",
      "  80 見る\t\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat case_patterns.txt | sort | uniq -c | sort -nr | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 443 する\tを\n",
      " 435 する\t\n",
      " 188 する\tに\n",
      " 134 する\tと\n",
      " 117 する\tが\n",
      " 110 する\tに を\n",
      " 102 する\tて を\n",
      "  88 する\tて\n",
      "  61 する\tは\n",
      "  60 する\tが を\n",
      "  51 する\tで を\n",
      "  50 する\tから\n",
      "  43 する\tで\n",
      "  42 する\tと を\n",
      "  42 する\tの\n",
      "  39 する\tも\n",
      "  38 する\tから を\n",
      "  31 する\tが に\n",
      "  29 する\tと は\n",
      "  28 する\tは を\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "grep -E \"^する\" case_patterns.txt | sort | uniq -c | sort -nr | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 179 見る\tて\n",
      "  94 見る\tを\n",
      "  80 見る\t\n",
      "  25 見る\tて て\n",
      "  20 見る\tから\n",
      "  16 見る\tて を\n",
      "  15 見る\tと\n",
      "  13 見る\tで\n",
      "  12 見る\tから て\n",
      "  10 見る\tて は\n",
      "   9 見る\tに\n",
      "   8 見る\tに を\n",
      "   7 見る\tが を\n",
      "   7 見る\tが\n",
      "   6 見る\tて は を\n",
      "   5 見る\tで を\n",
      "   5 見る\tて ば\n",
      "   5 見る\tて と\n",
      "   5 見る\tは\n",
      "   4 見る\tから を\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "grep -E \"^見る\" case_patterns.txt | sort | uniq -c | sort -nr | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3 与える\tに を\n",
      "   2 与える\tて に は を\n",
      "   1 与える\tけれども に を\n",
      "   1 与える\tじゃあ て は を\n",
      "   1 与える\tとして を\n",
      "   1 与える\tだけ で を\n",
      "   1 与える\tたり に を\n",
      "   1 与える\tに に対して も\n",
      "   1 与える\tて て と に は は を\n",
      "   1 与える\tて に に は を\n",
      "   1 与える\tて に を\n",
      "   1 与える\tば を\n",
      "   1 与える\tて を\n",
      "   1 与える\tけ に\n",
      "   1 与える\tが を\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "grep -E \"^与える\" case_patterns.txt | sort | uniq -c | sort -nr | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "- 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "- 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "```\n",
    "始める  で      ここで\n",
    "見る    は を   吾輩は ものを\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = defaultdict(list)\n",
    "with open('case_frames.txt', 'w', encoding='utf8') as f:\n",
    "    for sent in dependency:\n",
    "        for dep in sent:\n",
    "            if u'動詞' in [m.pos for m in dep.morphs]:\n",
    "                predicate = [m for m in dep.morphs if m.pos==u'動詞'][0].base\n",
    "                particles = []\n",
    "                chunks = []\n",
    "                for dependent in [sent[i] for i in dep.srcs]:\n",
    "                    particle = [m for m in dependent.morphs if m.pos==u'助詞']\n",
    "                    if particle:\n",
    "                        particles.append(particle[0].base)\n",
    "                        chunks.append(dependent.surface(sep=''))\n",
    "                string = '%s\\t%s' % (predicate, ' '.join(sorted(particles)))\n",
    "                string += '\\t%s' % (' '.join([c for p, c in sorted(zip(particles, chunks), key=lambda x: x[0])]))\n",
    "                f.write(string + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生れる\tで\tどこで\n",
      "つく\tか が\t生れたか 見当が\n",
      "する\t\t\n",
      "泣く\tで\t所で\n",
      "する\tだけ て\tいた事だけは 泣いて\n",
      "始める\tで\tここで\n",
      "見る\tは を\t吾輩は ものを\n",
      "聞く\tで\tあとで\n",
      "捕える\tを\t我々を\n",
      "煮る\tて\t捕えて\n",
      "食う\tて\t煮て\n",
      "思う\tから\tなかったから\n",
      "載せる\tに\t掌に\n",
      "持ち上げる\tて と\t載せられて スーと\n",
      "する\t\t\n",
      "ある\tが\t感じが\n",
      "落ちつく\tで\t上で\n",
      "見る\tて を\t落ちついて 顔を\n",
      "見る\tの\tものの\n",
      "思う\tと\tものだと\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "head -20 case_frames.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 47. 機能動詞構文のマイニング\n",
    "\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "\n",
    "- 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "- 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "- 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "- 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
    "\n",
    "```\n",
    "返事をする      と に は        及ばんさと 手紙に 主人は\n",
    "```\n",
    "\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "- コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "- コーパス中で頻出する述語と助詞パターン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = defaultdict(list)\n",
    "with open('verb_frames.txt', 'w', encoding='utf8') as f:\n",
    "    for sent in dependency:\n",
    "        for j, dep in enumerate(sent):\n",
    "            if j > 0 and u'動詞' in [m.pos for m in dep.morphs]:\n",
    "                prev = sent[j-1]\n",
    "                wo = prev.morphs[-1].surface\n",
    "                if prev.morphs[-1].surface == u'を' and prev.morphs[-2].pos1 == u'サ変接続':\n",
    "                    predicate = prev.morphs[-2].surface + prev.morphs[-1].surface\n",
    "                    predicate += [m.base for m in dep.morphs if m.pos == u'動詞'][0]\n",
    "                \n",
    "                    particles = []\n",
    "                    chunks = []\n",
    "                    for i in dep.srcs:\n",
    "                        dependent = sent[i]\n",
    "                        particle = [m for m in dependent.morphs if m.pos==u'助詞']\n",
    "                        if particle:\n",
    "                            particles.append(particle[0].base)\n",
    "                            chunks.append(dependent.surface(sep=''))\n",
    "                    string = '%s\\t%s' % (predicate, ' '.join(sorted(particles)))\n",
    "                    string += '\\t%s' % (' '.join([c for p, c in sorted(zip(particles, chunks), key=lambda x: x[0])]))\n",
    "                    f.write(string + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  30 返事をする\n",
      "  21 挨拶をする\n",
      "  16 話をする\n",
      "  14 真似をする\n",
      "  13 喧嘩をする\n",
      "   8 質問をする\n",
      "   7 運動をする\n",
      "   6 昼寝をする\n",
      "   5 質問をかける\n",
      "   5 相談をする\n",
      "   5 病気をする\n",
      "   5 注意をする\n",
      "   5 話を聞く\n",
      "   4 いたずらをする\n",
      "   4 休養を要する\n",
      "   4 降参をする\n",
      "   4 辞儀をする\n",
      "   4 演説をする\n",
      "   4 欠伸をする\n",
      "   4 散歩をする\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cut -f1 verb_frames.txt | sort | uniq -c | sort -nr | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   8 真似をする\tを\n",
      "   6 運動をする\tを\n",
      "   6 喧嘩をする\tを\n",
      "   4 挨拶をする\tから を\n",
      "   4 返事をする\tと を\n",
      "   4 返事をする\tを\n",
      "   4 話を聞く\tを\n",
      "   4 話をする\tを\n",
      "   3 返事をする\tと は を\n",
      "   3 挨拶をする\tと を\n",
      "   3 喧嘩をする\tと を\n",
      "   2 御無沙汰をする\tを\n",
      "   2 いたずらをする\tを\n",
      "   2 同情を表する\tて と は を\n",
      "   2 質問をかける\tと は を\n",
      "   2 休養を要する\tは を\n",
      "   2 深入りをする\tを\n",
      "   2 返事をする\tから と を\n",
      "   2 質問をする\tが を\n",
      "   2 議論をする\tて を\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cut -f1-2 verb_frames.txt | sort | uniq -c | sort -nr | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 48. 名詞から根へのパスの抽出\n",
    "\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 各文節は（表層形の）形態素列で表現する\n",
    "- パスの開始文節から終了文節に至るまで，各文節の表現を\"->\"で連結する\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "吾輩は -> 見た\n",
    "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "人間という -> ものを -> 見た\n",
    "ものを -> 見た\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩は -> 見た\n",
      "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
      "人間という -> ものを -> 見た\n",
      "ものを -> 見た\n"
     ]
    }
   ],
   "source": [
    "# recursively find the path\n",
    "def get_path(chunks, start, path):\n",
    "    head = chunks[start].dst\n",
    "    if head == -1:\n",
    "        return\n",
    "    else:\n",
    "        path.append(head)\n",
    "        get_path(chunks, head, path)\n",
    "\n",
    "sent = dependency[7]\n",
    "for j, dep in enumerate(sent):\n",
    "    if u'名詞' in [m.pos for m in dep.morphs]:\n",
    "        path = [j]\n",
    "        get_path(sent, j, path)\n",
    "        print(' -> '.join([sent[i].surface(sep='') for i in path]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 49. 名詞間の係り受けパスの抽出\n",
    "\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号が\\(i\\)と\\(j\\)（\\(i < j\\)）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "- 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を\"->\"で連結して表現する\n",
    "- 文節\\(i\\)と\\(j\\)に含まれる名詞句はそれぞれ，XとYに置換する\n",
    "- また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "  - 文節\\(i\\)から構文木の根に至る経路上に文節\\(j\\)が存在する場合: 文節\\(i\\)から文節\\(j\\)のパスを表示\n",
    "  - 上記以外で，文節\\(i\\)と文節\\(j\\)から構文木の根に至る経路上で共通の文節\\(k\\)で交わる場合:  \n",
    "    文節\\(i\\)から文節\\(k\\)に至る直前のパスと文節(j\\)から文節\\(k\\)に至る直前までのパス，文節\\(k\\)の内容を\"|\"で連結して表示\n",
    "\n",
    "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "```\n",
    "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
    "Xは | Yという -> ものを | 見た\n",
    "Xは | Yを | 見た\n",
    "Xで -> 始めて -> Y\n",
    "Xで -> 始めて -> 人間という -> Y\n",
    "Xという -> Y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
      "Xは | Yという -> ものを | 見た\n",
      "Xは | Yを | 見た\n",
      "Xで -> 始めて -> Yという\n",
      "Xで -> 始めて -> 人間という -> Yを\n",
      "Xという -> Yを\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def format_surface(morphs, n, i, j):\n",
    "    chunk = ''\n",
    "    for m in morphs:\n",
    "        if n == i and m.pos == u'名詞':\n",
    "            chunk += 'X'\n",
    "        elif n == j and m.pos == u'名詞':\n",
    "            chunk += 'Y'\n",
    "        elif m.pos != u'記号':\n",
    "            chunk += m.surface\n",
    "    return chunk\n",
    "    \n",
    "sent = dependency[7]\n",
    "for l, dep in enumerate(sent):\n",
    "    nouns = []\n",
    "    for l, dep in enumerate(sent):\n",
    "        if u'名詞' in [m.pos for m in dep.morphs]:\n",
    "            nouns.append(l)\n",
    "    \n",
    "pairs = combinations(nouns, 2)\n",
    "for i, j in pairs:\n",
    "    path_i = [i]\n",
    "    get_path(sent, i, path_i)\n",
    "    \n",
    "    if j in path_i:\n",
    "        chunks = []\n",
    "        for n in path_i:\n",
    "            if n <= j:\n",
    "                chunks.append(format_surface(sent[n].morphs, n, i, j))\n",
    "        print(' -> '.join(chunks))\n",
    "        \n",
    "    else:\n",
    "        path_j = [j]\n",
    "        get_path(sent, j, path_j)\n",
    "        k = min(set(path_i).intersection(path_j))\n",
    "        \n",
    "        chunks_i = []\n",
    "        for n in path_i:\n",
    "            if n < k:\n",
    "                chunks_i.append(format_surface(sent[n].morphs, n, i, j))\n",
    "            \n",
    "        chunks_j = []\n",
    "        for m in path_j:\n",
    "            if m < k:\n",
    "                chunks_j.append(format_surface(sent[m].morphs, m, i, j))\n",
    "        \n",
    "        print('%s | %s | %s' % (' -> '.join(chunks_i), \n",
    "                                ' -> '.join(chunks_j), \n",
    "                                format_surface(sent[k].morphs, k, i, j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
